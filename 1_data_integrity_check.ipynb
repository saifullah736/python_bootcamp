{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Optimiser: Job Posting Dataset - Integrity Check\n",
    "\n",
    "## Project Overview\n",
    "This analysis is for **Data Optimiser**, a fictional recruitment company that needs insights into job posting data to better understand market demands for data science roles.\n",
    "\n",
    "### Objective\n",
    "Perform a comprehensive data integrity check on the job posting dataset to identify and document data quality issues before proceeding with analysis.\n",
    "\n",
    "### Dataset Description\n",
    "The dataset contains job postings for three main data roles:\n",
    "- Data Scientists\n",
    "- Data Analysts  \n",
    "- Data Engineers\n",
    "\n",
    "Each record includes information about job requirements, company details, location, salary, and required skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('job_postings_dataset.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Number of records: {len(df)}\")\n",
    "print(f\"Number of features: {len(df.columns)}\")\n",
    "print(\"\\n=== COLUMN INFORMATION ===\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n=== FIRST 5 ROWS ===\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=== MISSING VALUES ANALYSIS ===\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percentage = (missing_data / len(df)) * 100\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "print(missing_summary[missing_summary['Missing Count'] > 0])\n",
    "\n",
    "# Visualize missing data\n",
    "plt.figure(figsize=(10, 6))\n",
    "missing_data[missing_data > 0].plot(kind='bar')\n",
    "plt.title('Missing Values by Column')\n",
    "plt.ylabel('Number of Missing Values')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate records\n",
    "print(\"=== DUPLICATE RECORDS ANALYSIS ===\")\n",
    "total_duplicates = df.duplicated().sum()\n",
    "print(f\"Total duplicate rows: {total_duplicates}\")\n",
    "\n",
    "# Check for duplicate job IDs (should be unique)\n",
    "duplicate_job_ids = df['job_id'].duplicated().sum()\n",
    "print(f\"Duplicate job IDs: {duplicate_job_ids}\")\n",
    "\n",
    "# Check for potential duplicates based on key fields\n",
    "key_fields = ['job_title', 'company', 'location', 'salary']\n",
    "potential_duplicates = df.duplicated(subset=key_fields, keep=False).sum()\n",
    "print(f\"Potential duplicates based on key fields: {potential_duplicates}\")\n",
    "\n",
    "if potential_duplicates > 0:\n",
    "    print(\"\\nSample potential duplicates:\")\n",
    "    duplicate_samples = df[df.duplicated(subset=key_fields, keep=False)].head(10)\n",
    "    print(duplicate_samples[['job_id'] + key_fields])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Consistency Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check categorical variables for consistency\n",
    "print(\"=== CATEGORICAL VARIABLES ANALYSIS ===\")\n",
    "\n",
    "categorical_columns = ['job_title', 'company_size', 'experience_level', 'work_arrangement', 'location']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    print(f\"\\n{col.upper()}:\")\n",
    "    value_counts = df[col].value_counts()\n",
    "    print(f\"Unique values: {len(value_counts)}\")\n",
    "    print(value_counts.head(10))\n",
    "    \n",
    "    # Check for potential data inconsistencies\n",
    "    if col == 'location':\n",
    "        # Check for location format inconsistencies\n",
    "        locations = df[col].unique()\n",
    "        problematic_locations = [loc for loc in locations if ',' not in loc or loc != loc.title()]\n",
    "        if problematic_locations:\n",
    "            print(f\"Potentially problematic location formats: {problematic_locations[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salary data quality analysis\n",
    "print(\"=== SALARY DATA QUALITY ANALYSIS ===\")\n",
    "\n",
    "# Basic salary statistics\n",
    "salary_stats = df['salary'].describe()\n",
    "print(\"Salary Statistics:\")\n",
    "print(salary_stats)\n",
    "\n",
    "# Check for unrealistic salary values\n",
    "min_reasonable_salary = 30000\n",
    "max_reasonable_salary = 200000\n",
    "\n",
    "low_salaries = df[(df['salary'] < min_reasonable_salary) & (df['salary'].notna())]\n",
    "high_salaries = df[(df['salary'] > max_reasonable_salary) & (df['salary'].notna())]\n",
    "\n",
    "print(f\"\\nSalaries below ${min_reasonable_salary:,}: {len(low_salaries)}\")\n",
    "if len(low_salaries) > 0:\n",
    "    print(\"Sample low salaries:\")\n",
    "    print(low_salaries[['job_title', 'salary', 'experience_level']].head())\n",
    "\n",
    "print(f\"\\nSalaries above ${max_reasonable_salary:,}: {len(high_salaries)}\")\n",
    "if len(high_salaries) > 0:\n",
    "    print(\"Sample high salaries:\")\n",
    "    print(high_salaries[['job_title', 'salary', 'experience_level']].head())\n",
    "\n",
    "# Visualize salary distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df['salary'].hist(bins=30, alpha=0.7)\n",
    "plt.title('Salary Distribution')\n",
    "plt.xlabel('Salary ($)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=df, x='job_title', y='salary')\n",
    "plt.title('Salary Distribution by Job Title')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Date Format Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check date format consistency\n",
    "print(\"=== DATE FORMAT ANALYSIS ===\")\n",
    "\n",
    "# Check posting_date formats\n",
    "print(\"Posting Date samples:\")\n",
    "print(df['posting_date'].head(10).tolist())\n",
    "\n",
    "print(\"\\nDate Posted samples:\")\n",
    "print(df['date_posted'].head(10).tolist())\n",
    "\n",
    "# Try to identify different date formats\n",
    "date_formats_posting = df['posting_date'].apply(lambda x: len(x.split('-')) if '-' in str(x) else len(x.split('/')))\n",
    "date_formats_posted = df['date_posted'].apply(lambda x: len(x.split('-')) if '-' in str(x) else len(x.split('/')))\n",
    "\n",
    "print(\"\\nPosting date format distribution:\")\n",
    "print(date_formats_posting.value_counts())\n",
    "\n",
    "print(\"\\nDate posted format distribution:\")\n",
    "print(date_formats_posted.value_counts())\n",
    "\n",
    "# Check for inconsistent date ranges\n",
    "try:\n",
    "    df['posting_date_clean'] = pd.to_datetime(df['posting_date'])\n",
    "    print(f\"\\nDate range: {df['posting_date_clean'].min()} to {df['posting_date_clean'].max()}\")\n",
    "    \n",
    "    # Check for future dates\n",
    "    future_dates = df[df['posting_date_clean'] > datetime.now()]\n",
    "    print(f\"Job postings with future dates: {len(future_dates)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error parsing dates: {e}\")\n",
    "    print(\"This indicates inconsistent date formats that need cleaning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Skills Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze skills data structure\n",
    "print(\"=== SKILLS DATA ANALYSIS ===\")\n",
    "\n",
    "# Check skills format\n",
    "print(\"Sample skills entries:\")\n",
    "for i, skills in enumerate(df['required_skills'].head(5)):\n",
    "    print(f\"{i+1}. {skills}\")\n",
    "\n",
    "# Count number of skills per job\n",
    "df['num_skills'] = df['required_skills'].apply(lambda x: len(x.split(', ')) if pd.notna(x) else 0)\n",
    "\n",
    "print(f\"\\nSkills per job statistics:\")\n",
    "print(df['num_skills'].describe())\n",
    "\n",
    "# Most common skills across all jobs\n",
    "all_skills = []\n",
    "for skills_str in df['required_skills'].dropna():\n",
    "    skills_list = [skill.strip() for skill in skills_str.split(',')]\n",
    "    all_skills.extend(skills_list)\n",
    "\n",
    "from collections import Counter\n",
    "skill_counts = Counter(all_skills)\n",
    "\n",
    "print(\"\\nTop 15 most required skills:\")\n",
    "for skill, count in skill_counts.most_common(15):\n",
    "    print(f\"{skill}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Visualize skills distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_skills = dict(skill_counts.most_common(10))\n",
    "plt.bar(top_skills.keys(), top_skills.values())\n",
    "plt.title('Top 10 Most Required Skills')\n",
    "plt.xlabel('Skills')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Quality Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive data quality report\n",
    "print(\"=== DATA QUALITY SUMMARY REPORT ===\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Dataset overview\n",
    "print(f\"📊 DATASET OVERVIEW\")\n",
    "print(f\"   Total Records: {len(df):,}\")\n",
    "print(f\"   Total Features: {len(df.columns)}\")\n",
    "print(f\"   Memory Usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "\n",
    "# Data completeness\n",
    "print(f\"\\n📋 DATA COMPLETENESS\")\n",
    "completeness = (1 - df.isnull().sum() / len(df)) * 100\n",
    "for col in df.columns:\n",
    "    print(f\"   {col}: {completeness[col]:.1f}% complete\")\n",
    "\n",
    "# Data consistency issues\n",
    "print(f\"\\n⚠️  DATA QUALITY ISSUES IDENTIFIED\")\n",
    "issues = []\n",
    "\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    issues.append(f\"Missing Values: {df.isnull().sum().sum()} total missing values\")\n",
    "\n",
    "if df.duplicated().sum() > 0:\n",
    "    issues.append(f\"Duplicate Records: {df.duplicated().sum()} duplicate rows\")\n",
    "\n",
    "# Check for salary outliers\n",
    "salary_q1 = df['salary'].quantile(0.25)\n",
    "salary_q3 = df['salary'].quantile(0.75)\n",
    "salary_iqr = salary_q3 - salary_q1\n",
    "salary_outliers = len(df[(df['salary'] < salary_q1 - 1.5*salary_iqr) | (df['salary'] > salary_q3 + 1.5*salary_iqr)])\n",
    "if salary_outliers > 0:\n",
    "    issues.append(f\"Salary Outliers: {salary_outliers} potential outliers\")\n",
    "\n",
    "# Check for date inconsistencies\n",
    "if len(df['date_posted'].apply(lambda x: '-' if '-' in str(x) else '/').unique()) > 1:\n",
    "    issues.append(\"Date Format Inconsistency: Multiple date formats detected\")\n",
    "\n",
    "if issues:\n",
    "    for issue in issues:\n",
    "        print(f\"   ❌ {issue}\")\n",
    "else:\n",
    "    print(\"   ✅ No major data quality issues detected\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\n💡 RECOMMENDATIONS FOR DATA CLEANING\")\n",
    "print(\"   1. Standardize date formats to YYYY-MM-DD\")\n",
    "print(\"   2. Handle missing salary values (imputation or exclusion)\")\n",
    "print(\"   3. Remove or investigate duplicate records\")\n",
    "print(\"   4. Standardize location format (Title Case, consistent punctuation)\")\n",
    "print(\"   5. Validate salary ranges against industry benchmarks\")\n",
    "print(\"   6. Standardize company names for consistency\")\n",
    "\n",
    "print(f\"\\n✅ DATA INTEGRITY CHECK COMPLETE\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Power BI Translation Notes\n",
    "\n",
    "### For Power BI Implementation:\n",
    "\n",
    "**Power Query Steps:**\n",
    "1. **Data Import**: Use `Data > Get Data > Text/CSV` to import the job_postings_dataset.csv\n",
    "2. **Data Type Detection**: Power Query will auto-detect data types, verify and adjust as needed\n",
    "3. **Missing Value Handling**: Use `Transform > Replace Values` to handle null values\n",
    "4. **Date Standardization**: Use `Transform > Date > Parse` to standardize date formats\n",
    "5. **Duplicate Removal**: Use `Home > Remove Duplicates` based on key columns\n",
    "6. **Data Validation**: Use `Data > Column Quality` to visualize data quality metrics\n",
    "\n",
    "**DAX Measures for Data Quality:**\n",
    "- `Data Quality Score = DIVIDE(COUNTROWS(FILTER(JobData, NOT(ISBLANK([salary])))), COUNTROWS(JobData))`\n",
    "- `Missing Salary Count = COUNTBLANK(JobData[salary])`\n",
    "- `Duplicate Records = COUNTROWS(JobData) - DISTINCTCOUNT(JobData[job_id])`\n",
    "\n",
    "**Next Steps:**\n",
    "After completing this integrity check, proceed to:\n",
    "1. Exploratory Data Analysis (EDA)\n",
    "2. Business Question Investigation\n",
    "3. Dashboard Creation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}